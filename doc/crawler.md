# クローリングの流れ

## 各サーバを回るcrawler_main

1. 初期設定
   1. 設定の読み込み
   2. 使うヘッドレスブラウザを取得
   3. parseに時間のかかるスレッドを監視するスレッドの作成
2. クローリングループ開始
   1. 親にどのURLをクロールすればいいか聞く
   2. ページオブジェクトの作成
   3. robots.txtのチェック
   4. urlopenで接続
   5. リダイレクトチェック
   6. content-type からwebページかその他のファイルかチェック
   7. WebPageに対する処理
      1. ヘッドレスブラウザで再接続
      2. もしrobots.txtでクロールだめにされてたら諦めて次
      3. about:blankのウィンドウを作成
      4. watcher_extentionを開始する
      5. リソース監視スレッドの開始
         1. **DAEMON にしてるけどよくないぞ無限にスレッド作られるぞ**
         2. **crawler_main.py で os.getpid() してるけどwebdriverのpid多分違う**
      6. ブラウザからのHTML取得
         1. 取得に失敗したらWebdriverを作り直す
      7. watcherを終了してwatcher.htmlのデータを抜き取る
      8. リダイレクト等のチェック
      9. ページのHTMLの内容を精査するスレッドを作成
      10. 検査済みURL数を増やす
   8. WebPage以外のとき
      1. ハッシュ値の比較
      2. clamdで調査
      3. 検査済みURLを増やす
   9. 終了処理
      1. 100URLクローリングしたら抜ける
      2. 時間をオーバーしたら抜ける
3. エラーでループを抜けた場合
   1. url_chaceにデータを保存して奥
4. データの保存
5. WebDriverの終了
6. プロセスの終了
   1. **os._exit()してるけどそんな強力なのいる???**
